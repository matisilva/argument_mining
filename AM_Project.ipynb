{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Pongamos a trabajar las redes neuronales de moda!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "Basándonos en los repositorios de [UKPLab](https://github.com/UKPLab):\n",
    "\n",
    "- https://github.com/UKPLab/acl2017-neural_end2end_am (Argument minning con Lasagne)\n",
    "- https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf (Etiquetado de secuencias)\n",
    "\n",
    "\n",
    "- Introducirnos en las redes neuronales usando el framework *Keras* conociendo sus distintos backends.\n",
    "- Conocer las diversas capas y sus utilidades.\n",
    "- Entender la arquitectura del proyecto actual.\n",
    "\n",
    "Adaptar el código para..\n",
    "- Utilizar la última version de Keras y TF para poder utilizar CNNs sin necesidad de usar otro backend.\n",
    "- Desarrollar un entorno para la identificación y etiquetado de argumentos en textos del idioma inglés.\n",
    "- Proponer alternativas de embeddings distintos para entrenamiento/evaluación.\n",
    "- Flexibilizar input para formato texto o formato CoNLL.\n",
    "- Reportar resultados con diversos parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿De qué se trata el proyecto aparte de aprender a usar Keras y TF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo principal de la tarea es el **correcto etiquetado de argumentos en ensayos**. La identificación de argumentos conocido como **Argument Mining** tiene la particularidad de necesitar un **contexto muy amplio**, es decir necesitan recordar durante todo el documento detalles previos para un correcto funcionamiento.\n",
    "\n",
    "Como muchas otras tareas semánticas del proceso de lenguaje que necesitan un gran contexto se proponen **redes neuronales con memoria**, es decir no simplemente una CNN sino algo que conserve una memoria mas allá de la iteración. Éstas son las llamadas **LSTM**(Long Short-Term Memory). \n",
    "\n",
    "Pero si sólo nos quedáramos con esto no aprenderíamos del pasado, es por eso que dicha red deberá ser Bidireccional teniendo así una red **BiLSTM**\n",
    "\n",
    "### En resumen:\n",
    "Combinando una red neuronal BiLSTM con diversos ajustes, con otras capas para realizar la tarea de detección de argumentos en ensayos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo de lo que aprendimos...  ``from keras.layers.import * ``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve descripción de cada capa y su utilidad.\n",
    "\n",
    "- **CNN**(Convolutional Neuronal Network): Intenta buscar patrones similares en el espacio de los datos de entrada. Ideal para la detección de patrones en imágenes, tales como bordes colores, formas, etc. No tiene en cuenta contexto a la hora de detectar un patrón.\n",
    "\n",
    "- **RNN**(Recurrent Neuronal Network): Similar a una capa convolucional pero orientada a detectar patrones similares en el tiempo, ideal para procesamiento de texto debido a que de esta forma se basa en el contexto para tomar una determinación. Por ejemplo, en el caso de traduccion automática, en el caso de \"dog\" tendrá en cuenta si es precedida por \"hot\".\n",
    "\n",
    "- **LSTM**(Long Short-Term Memory): A diferencia de las demás, estas poseen una memoria que es olvidada en un tiempo arbitrario. De esta forma logran tener mucho mas contexto en las decisiones.\n",
    "\n",
    "- **Embedding**: Simplemente transforma los datos de entrada en un embedding que será la entrada de la siguiente capa.\n",
    "\n",
    "- **Timedistributed**: Realiza una partición temporal de los datos de entrada, dandole a la siguiente capa los datos fraccionados por lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra Arquitectura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](arch.jpeg)\n",
    "```\n",
    "________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "========================================================================================\n",
    "token_emd_input (InputLayer)    (None, None)         0                                            \n",
    "_______________________________________________________________________________________\n",
    "casing_emd_input (InputLayer)   (None, None)         0                                            \n",
    "_______________________________________________________________________________________\n",
    "token_emd (Embedding)           (None, None, 300)    52205700    token_emd_input[0][0]            \n",
    "_______________________________________________________________________________________\n",
    "casing_emd (Embedding)          (None, None, 8)      64          casing_emd_input[0][0]           \n",
    "_______________________________________________________________________________________\n",
    "concatenate_1 (Concatenate)     (None, None, 308)    0           token_emd[0][0]                  \n",
    "                                                                 casing_emd[0][0]                 \n",
    "________________________________________________________________________________________\n",
    "main_LSTM_1 (Bidirectional)     (None, None, 250)    434000      concatenate_1[0][0]              \n",
    "________________________________________________________________________________________\n",
    "softmax_output (TimeDistributed (None, None, 7)      1757        main_LSTM_1[0][0]                \n",
    "========================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parece una tarea sencilla.. Veamos las piedras en el camino..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¡**Que difícil es leer codigo ajeno..**!\n",
    "\n",
    "- **Primer output**: Con un accuracy de 50% y al correr el modelo solo etiqueta MajorClaims... ¿RunModel tiene un bug o la efectividad reportada no es correcta? Ambos..\n",
    "\n",
    "- **Métricas por clase**: Calculamos en cada epoch de entrenamiento la evolución de todas sus categorias.\n",
    "\n",
    "```\n",
    "[Claim:Against]: Prec: 0.000, Rec: 0.000, F1: 0.0000\n",
    "[O]: Prec: 0.833, Rec: 0.414, F1: 0.5527\n",
    "[Premise:Support]: Prec: 0.492, Rec: 0.960, F1: 0.6504\n",
    "[Claim:For]: Prec: 0.000, Rec: 0.000, F1: 0.0000\n",
    "[Premise:Attack]: Prec: 0.000, Rec: 0.000, F1: 0.0000\n",
    "[Claim:Support:For]: Prec: 0.000, Rec: 0.000, F1: 0.0000\n",
    "[MajorClaim]: Prec: 0.000, Rec: 0.000, F1: 0.0000\n",
    "\n",
    "Max: 0.1719 on dev;\n",
    "```\n",
    "\n",
    "- **Archivos de etiquetado preliminares de cada epoch**: Generamos archivos preliminares de cada epoch siempre que la métrica F1 se incremente. ``/tmp/[f1_score]_[test o dev].txt\"``\n",
    "\n",
    "- **EarlyStopping no es buena idea**: Desactivamos la heurística de dejar de entrenar luego de que en 5 epochs no se genere ninguna mejora. El modelo sigue mejorando..\n",
    "\n",
    "- **Probemos simplificar las etiquetas**: Mediante un script de python simplificamos las etiquetas (Claim, Premise, etc) en un modelo más simple que sólo diferencia 4 etiquetas(am_simplest).\n",
    "\n",
    "- **Llegó Keras 2**: Upgrade de entorno a las últimas versiones de Keras y TF. Ahora podemos usar CNN con TF!\n",
    "\n",
    "```\n",
    "_______________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "=======================================================================================\n",
    "token_emd (Embedding)           (None, None, 300)    52205700                \n",
    "_______________________________________________________________________________________\n",
    "casing_emd (Embedding)          (None, None, 8)      64          \n",
    "_______________________________________________________________________________________\n",
    "main_LSTM_1 (Bidirectional)     (None, None, 250)    434000      merge_1[0][0]              \n",
    "_______________________________________________________________________________________\n",
    "softmax_output (TimeDistributed (None, None, 7)      1757        main_LSTM_1[0][0]                \n",
    "=======================================================================================\n",
    "\n",
    "\n",
    "_______________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "=======================================================================================\n",
    "token_emd_input (InputLayer)    (None, None)         0                                            \n",
    "_______________________________________________________________________________________\n",
    "casing_emd_input (InputLayer)   (None, None)         0                                            \n",
    "_______________________________________________________________________________________\n",
    "token_emd (Embedding)           (None, None, 300)    52205700    token_emd_input[0][0]            \n",
    "_______________________________________________________________________________________\n",
    "casing_emd (Embedding)          (None, None, 8)      64          casing_emd_input[0][0]           \n",
    "_______________________________________________________________________________________\n",
    "concatenate_1 (Concatenate)     (None, None, 308)    0           token_emd[0][0]                  \n",
    "                                                                 casing_emd[0][0]                 \n",
    "_______________________________________________________________________________________\n",
    "main_LSTM_1 (Bidirectional)     (None, None, 250)    434000      concatenate_1[0][0]              \n",
    "_______________________________________________________________________________________\n",
    "softmax_output (TimeDistributed (None, None, 7)      1757        main_LSTM_1[0][0]                \n",
    "=======================================================================================\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Algunos resultados\n",
    "\n",
    "| am_simplest \t              | f1 promedio(dev) | f1 promedio(test) | epochs model |\n",
    "|-----------------------------|------------------|-------------------|--------------|\n",
    "| crf-adam             \t      | 0.71             | 0.71              | 34 epochs    |\n",
    "| softmax-nadam               | 0.72             | 0.73              | 20 epochs    |\n",
    "| softmax-sgd                 | 0.48             | 0.50           \t | 44 epochs    |\n",
    "| crf-nadam (levy)            | 0.71             | 0.70        \t     | 32 epochs    |\n",
    "| softmax-nadam (glove 100d)  | 0.69             | 0.70        \t     | 34 epochs    |\n",
    "| softmax-nadam-paragraph     | 0.70             | 0.72        \t     | 16 epochs    |\n",
    "| softmax-nadam-paragraph(cnn)| 0.71             | 0.74        \t     | 27 epochs    |\n",
    "\t\n",
    "\n",
    "| am_full (levy)              | f1 promedio(dev) | f1 promedio(test) | epochs model |\n",
    "|-----------------------------|------------------|-------------------|--------------|\n",
    "| charEmbedding(lstm)-crf     | 0.46             | 0.68              | 20 epochs    |\n",
    "| charEmbedding(cnn)-crf      | 0.48             | 0.46              | 39 epochs    |\n",
    "| charEmbeddings(cnn)-softmax | 0.46             | 0.71              | 23 epochs    |\n",
    "| softmax-paragraph(cnn)      | 0.51             | 0.56              | 26 epochs    |\n",
    "| softmax                     | 0.44             | 0.47              | 49 epochs    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo relacionado\n",
    "\n",
    "## Input\n",
    "\n",
    "- Añadir más información de interés en el ingreso a la red. (ngrams, perfeccionar el casing, posición dentro de la oración, etc)\n",
    "\n",
    "- Entrenar un embedding propio y utilizarlo para la vectorización de palabras.\n",
    "\n",
    "- Añadir temática o tópico del ensayo como feature.\n",
    "\n",
    "## Red\n",
    "\n",
    "- Si bien las funciones de pérdida exigidas en esta red son de tipo **sparse** adaptar para utilizar alguna otra funcion de perdida\n",
    "\n",
    "- Probar resultados con solo una simple RNN retirando la capa LSTM para justificar una mejora en el uso de una LSTM.\n",
    "\n",
    "- Añadir Attention en LSTM\n",
    "\n",
    "## Output\n",
    "\n",
    "- Permitir distintas salidas configurables por usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos conceptos de interés\n",
    "-  [¿Como crear un modelo simple con Keras y TF?](BuildModel.ipynb)\n",
    "-  [RunModel es un script para correr un modelo pero..¿Que hace el script?](RunModel.ipynb)\n",
    "-  [Informe final](https://docs.google.com/document/d/1PGZu9Et5rjSjlU7PbYnwPcyE4B0-yQDZkN372czhV5o/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am_python2.7",
   "language": "python",
   "name": "am_python2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
